---
title: "ABC"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning = FALSE)
pacman::p_load(dplyr,ggplot2,tidyr,brms,janitor,vroom,hrbrthemes,deSolve,furrr,purrr)
```

# ABC vanilla

## Import the data


```{r data import}
df<-vroom::vroom("../Data/gerba.data.20200302.csv") %>% janitor::clean_names()  %>% 
  mutate(treatment=as.factor(treatment),
         group=as.factor(group))
df 
```


## Distance function

You can include R code in the document as follows:

```{r distance}
Distance<-function(x,y,s){  # computes the Euclidean distance between two lists of the same length
  if (length(x) == length(y)){
    sqrt(sum(((x)-(y))/(s))^2)
              }
  else{
    print( 'lists not the same length')}
}
```

## ODE solver function

```{r ODE function}
ode_model <- function(time_space, initial_contamination, parameters){
  with(
    as.list(c(initial_contamination, parameters)),{
      # dContamination <- Contamination*r*(1-Contamination/C)-d*exp(-g*time_space)*Contamination
      dContamination <- (1-Contamination/C)*r-d*exp(-g*time_space)*Contamination
      return(list(dContamination))
    }
  )
}
```

## Deterministic run function

```{r deterministic run function}

deterministic_run<-function(precision,initial_contamination,parameters){
  tmax = 24
  time_space = seq(0, tmax, length.out = precision+1)

#sim=odeint(ode_model,initial_contamination,time_space,args=(r,C,d,g,l))
#parameters=c(r,C,d,g,l)
sim<- ode(initial_contamination, time_space, ode_model, parameters, method = "radau",atol = 1e-4, rtol = 1e-4)
# num_at_0=sim[int(precision*0.1/50.0)]
# num_at_1=sim[(precision*1/50.0),2]
num_at_2=sim[(precision*2/50.0),2]
num_at_4=sim[(precision*4/50.0),2]
num_at_6=sim[(precision*6/50.0),2]
num_at_16=sim[(precision*16/50.0),2]
num_at_18=sim[(precision*18/50.0),2]
num_at_20=sim[(precision*20/50.0),2]
num_at_22=sim[(precision*22/50.0),2]
num_at_24=sim[(precision*24/50.0),2]


return(cbind(num_at_2,num_at_4,num_at_6,num_at_16,num_at_18,num_at_20,num_at_22,num_at_24))
}
```

Initial data to input into the algorithm
```{r}
df %>% 
  group_by(treatment,group,time) %>% 
  summarise(M=mean(conc),S=sd(conc))
  filter(treatment=="Treated") %>% 
  select(time) 
```



# Run the ABC Rejection Code

```{r}
##################################################################################################################
## Applying the ABC rejection algorithm
initial_contamination=c(Contamination=1200) #median 34
experimental_data = c(134,202,294,400,644,1232,2044,2868)#c(11.5,5,2,0,6)#
s=c(93.70165,86.13942,162.11107,116.61904,123.61230,396.88789,628.87201,1147.13556)
sample_size = 1000
#parameter_sample <- c()
parameter_sample<- matrix(data=NA,nrow=1,ncol=5)
total_trials=0  #Starts the counter of run trials
accepted_trials=0 #Starts the counter for accepted trial numbers, don't change


#Create empty vector for the distances
distances<-c()

precision=5000 #this is delta t
# library(foreach)
# library(doParallel)
# cores=detectCores()
# cl <- makeCluster(cores[1]-1) #not to overload your computer
# registerDoParallel(cl)
while (NROW(parameter_sample) < sample_size){
#foreach(total_trials=1:2, .packages=c("deSolve")) %dopar% {
  
  # The prior distributions we use are d ~ U(0.001,10.0), C ~ U(200,1200), r ~ U(0.001,1.0), g ~ U(0.001,1.0). 
  # We begin by sampling from these distributions and simulating the process
trial_r = runif(1,1E-1,1e2)
trial_C = runif(1,1,3e3)
trial_d = runif(1,1,1e2)
trial_g = runif(1,1E-2,1e2)
trial_l = runif(1,1E-5,1e2)
total_trials=total_trials+1.0
#print(total_trials)
parameters=c(r=trial_r,C=trial_C,d=trial_d,g=trial_g,l=trial_l)


one_run = deterministic_run(precision,initial_contamination,parameters)#

#experimental_data_noP2 = [652.0, 556.0, 424.5, 467.5, 428.0, 550.0, 672.0]

# Now we find the Euclidean distance between the simulated output and the
# experimental results. delta is the threshold that the Euclidean distance
# must be less than for us to accept the trial parameters into our sample.
delta = 1 #dictates distance
euclidean_distance = Distance(one_run, experimental_data,s)#abs(dist(rbind(one_run,experimental_data))) #Distance(one_run, experimental_data,s)#_noP2)
#print(euclidean_distance)# print(parameter_sample,euclidean_distance)
#print(total_trials)

if (euclidean_distance < delta){
  #parameter_sample = parameter_sample[!is.na(parameter_sample)];
  parameter_sample=rbind(parameter_sample, c(trial_r,trial_C,trial_d,trial_g,trial_l))
  distances=rbind(distances,euclidean_distance)
  accepted_trials=accepted_trials+1.0
  print(paste0("Trial number accepted: ",accepted_trials))
}
  #else{
  #  print(euclidean_distance)
  #  }

}


print(paste0("Percentage of trials accepted: ",(100*accepted_trials/total_trials)))
#Order the distances and then the parameter sample
parameter_sample<-parameter_sample[order(distances), ]
parameter_sample<-parameter_sample%>%as.data.frame()#%>%set_colnames(c("r", "C", "d","g","l"))
colnames(parameter_sample)<-c("r", "C", "d","g","l")
#stopImplicitCluster()


write.csv(parameter_sample,"../Data/parameter_sample.csv",row.names = FALSE)
write.csv(distances,"../Data/distances.csv",row.names = FALSE)

```

This is the parallel implementation usingg the furrrr framework with 4 worker cores
```{r parallel implementation with furrr}
##################################################################################################################
## Applying the ABC rejection algorithm

# library(foreach)
# library(doParallel)
# cores=detectCores()
# cl <- makeCluster(cores[1]-1) #not to overload your computer
# registerDoParallel(cl)

simulate_function<-function(a){
  # set.seed(seed = TRUE)
  .Random.seed <- a
  #set up empty distances vector
  distances<-c()
  precision=5000 #this is delta t
  
  #Experimental data
  initial_contamination=c(Contamination=1200) #median 34
  experimental_data = c(134,202,294,400,644,1232,2044,2868)#c(11.5,5,2,0,6)#
  s=c(93.70165,86.13942,162.11107,116.61904,123.61230,396.88789,628.87201,1147.13556)
  sample_size = 100
  #parameter_sample <- c()
  parameter_sample<- matrix(data=NA,nrow=1,ncol=5)
  total_trials=0  #Starts the counter of run trials
  accepted_trials=0 #Starts the counter for accepted trial numbers, don't change


  #Loop to sample values until met the sample size
while (NROW(parameter_sample) <= sample_size){
#foreach(total_trials=1:2, .packages=c("deSolve")) %dopar% {
  
  # The prior distributions we use are d ~ U(0.001,10.0), C ~ U(200,1200), r ~ U(0.001,1.0), g ~ U(0.001,1.0). 
  # We begin by sampling from these distributions and simulating the process
trial_r = runif(1,1E-1,1e2)
trial_C = runif(1,1,3e3)
trial_d = runif(1,1,1e2)
trial_g = runif(1,1E-2,1e2)
trial_l = runif(1,1E-5,1e2)
total_trials=total_trials+1.0
#print(total_trials)
parameters=c(r=trial_r,C=trial_C,d=trial_d,g=trial_g,l=trial_l)


one_run = deterministic_run(precision,initial_contamination,parameters)#

#experimental_data_noP2 = [652.0, 556.0, 424.5, 467.5, 428.0, 550.0, 672.0]

# Now we find the Euclidean distance between the simulated output and the
# experimental results. delta is the threshold that the Euclidean distance
# must be less than for us to accept the trial parameters into our sample.
delta = 1 #dictates distance
euclidean_distance = Distance(one_run, experimental_data,s)#abs(dist(rbind(one_run,experimental_data))) #Distance(one_run, experimental_data,s)#_noP2)
#print(euclidean_distance)# print(parameter_sample,euclidean_distance)
#print(total_trials)

if (euclidean_distance < delta){
  #parameter_sample = parameter_sample[!is.na(parameter_sample)];
  parameter_sample=rbind(parameter_sample, c(trial_r,trial_C,trial_d,trial_g,trial_l))
  distances=rbind(distances,euclidean_distance)
  accepted_trials=accepted_trials+1.0
  print(paste0("Trial number accepted: ",accepted_trials))
}
  #else{
  #  print(euclidean_distance)
  #  }

}
  
parameter_sample<-parameter_sample[order(distances), ]
parameter_sample<-parameter_sample%>%as.data.frame()#%>%set_colnames(c("r", "C", "d","g","l"))
colnames(parameter_sample)<-c("r", "C", "d","g","l")
# return(parameter_sample)
return(list("parameter_sample" = parameter_sample, "distances" = distances))
}

# Parallel implementation

# print(paste0("Percentage of trials accepted: ",(100*accepted_trials/total_trials)))
# #Order the distances and then the parameter sample
# parameter_sample<-parameter_sample[order(distances), ]
# parameter_sample<-parameter_sample%>%as.data.frame()#%>%set_colnames(c("r", "C", "d","g","l"))
# colnames(parameter_sample)<-c("r", "C", "d","g","l")

#2 %>% purrr::rerun()
# Run in parallel

plan(multisession, workers = 4)
# temp.list<-lapply(X = 1:4,FUN = simulate_function)

RNGkind("L'Ecuyer-CMRG")
set.seed(42)
seeds <- list(.Random.seed)
temp.list<-future_map(.x = 1:4,.f = simulate_function)

# Reset to running sequentially
plan(sequential)


temp.list


#write.csv(parameter_sample,"parameter_sample_Alcohol.csv",row.names = FALSE)

```

# Extract distances and parameter samples
```{r extract parameter samples}
temp.list %>% 
  bind_rows() %>% 
  select(parameter_sample) %>% 
  unnest() %>% 
  drop_na() %>% 
   pivot_longer(c(r,C,d,g,l)) %>% 
  ggplot()+
  geom_histogram(aes(x=value,fill=name))+
  facet_wrap(~name,scales="free")+
  hrbrthemes::theme_ipsum()

temp.list %>% 
  bind_rows() %>% 
  select(parameter_sample) %>% 
  unnest() %>% 
  drop_na() ->parameter_sample
```


# Plot histograms
```{r}
parameter_sample %>% 
  pivot_longer(c(r,C,d,g,l)) %>% 
  ggplot()+
  geom_histogram(aes(x=value,fill=name))+
  facet_wrap(~name,scales="free")+
  hrbrthemes::theme_ipsum()

parameter_sample %>% summary()

write.csv(parameter_sample,"../Data/parameter_sample.csv",row.names = FALSE)
```


# Plot example curves against experimetnal data
```{r}
## Plotting a single best curve
  initial_contamination=c(Contamination=1200) #median 34
  experimental_data = c(134,202,294,400,644,1232,2044,2868)#c(11.5,5,2,0,6)#
  s=c(93.70165,86.13942,162.11107,116.61904,123.61230,396.88789,628.87201,1147.13556)

tmax = 24
time_space = seq(0, tmax, length.out = 5000+1)

sim1<-ode(initial_contamination, time_space, ode_model, parameter_sample[1,], method = "radau",atol = 1e-4, rtol = 1e-4)
sim2<-ode(initial_contamination, time_space, ode_model, parameter_sample[2,], method = "radau",atol = 1e-4, rtol = 1e-4)
sim3<-ode(initial_contamination, time_space, ode_model, parameter_sample[3,], method = "radau",atol = 1e-4, rtol = 1e-4)
sim4<-ode(initial_contamination, time_space, ode_model, parameter_sample[4,], method = "radau",atol = 1e-4, rtol = 1e-4)
sim<-rbind(sim1[,c(1,2)],sim2[,c(1,2)],sim3[,c(1,2)],sim4[,c(1,2)])%>%as.data.frame()
sim$CurveNum<-rep(1:4,each=nrow(sim1))
#lines(sim,col="blue")
sim<-sim%>%as.data.frame()#%>%tidyr::set_colnames(c("Time","Contamination","CurveNum"))
colnames(sim)<-c("Time","Contamination","CurveNum")
# df<- read_csv("~/Downloads/SurfaceCleaning/Data/cleaningExptBeth.csv")#data.frame(Time=c(0,1,2,4,8,24),Contamination=c(initial_contamination,experimental_data))
# df<-df[,-2]%>%set_colnames(c("Contamination","CleaningType","Time","Replica"))
# df$CleaningType<-factor(df$CleaningType, levels = c("Control","Alcohol","Detergent", "Distilled Water"))


ggplot()+
  geom_line(data=sim %>% as_tibble(),aes(x=Time,y=Contamination,colour=as.factor(CurveNum)))+
  geom_point(data=df %>% filter(treatment=="Treated"),aes(x=time,y=conc))+
  scale_y_continuous(trans="log10")+
  hrbrthemes::theme_ipsum()+
   theme(legend.position = "none")

distances %>% as_tibble %>% 
  separate(V1,into = c(NA,"V1"),sep=" ")
```

