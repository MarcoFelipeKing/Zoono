---
title: "ABC rejection algorithm"
output: github_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning = FALSE)
pacman::p_load(dplyr,ggplot2,tidyr,janitor,vroom,hrbrthemes,deSolve,furrr,ggpubr)

# Check the system architecture before loading the compiled function

if(Sys.info()["machine"]=="arm64"){
  system("R CMD SHLIB testODE_arm64.c")
  dyn.load("testODE_arm64.so")  
}else{
  system("R CMD SHLIB testODE_X86.c")
  dyn.load("testODE_X86.so")  
}


dynLoad <- function(dynlib){
    dynlib <- paste(dynlib, .Platform$dynlib.ext, sep = "")
    dyn.load(dynlib)
}
dynLoad("testODE_arm64")
# dynUnload <- function(dynlib){
#     dynlib <- paste(dynlib, .Platform$dynlib.ext, sep = "")
#     dyn.unload(dynlib)
# }
```

# ABC vanilla

## Import the data


```{r data import, echo=FALSE,warning=FALSE}
df<-vroom::vroom("../Data/gerba.data.20200302.csv") %>% janitor::clean_names()  %>% 
  mutate(treatment=as.factor(treatment),
         group=as.factor(group))
```
## Plot the data
```{r}
df %>% 
  ggplot()+
  geom_point(aes(x=time,y=conc,colour=treatment))+
  scale_y_continuous(trans = "log10")+
  xlab("Time [h]")+
  ylab("CFUs")+
  hrbrthemes::theme_ipsum()
  
```

## Distance function

Here we calculate the squared Euclidean distance (ignoring the square root as this is very slow). Other distances could be using summary statistics including quantiles

```{r distance}
Distance<-function(x,y,s){  # computes the Euclidean distance between two lists of the same length
  if (length(x) == length(y)){
     #sqrt(sum((x-y)/(s))^2) #sqrt is slow
    (sum((x-y)/s))^2 #This is faster than abs(dist(rbind(x/s,y/s))) by 1 order of magnitude
  }
  else{
    print( 'vectors are not the same length')}
}
```

## ODE solver function

```{r ODE function}
# ode_model <- function(times, initial_contamination, parameters){
#   with(
#     as.list(c(initial_contamination, parameters)),{
#       # dContamination <- Contamination*r*(1-Contamination/C)-d*exp(-g*times)*Contamination
#       dContamination <- (1-Contamination/C)*r-d*exp(-g*times)*Contamination
#       return(list(dContamination))
#     }
#   )
# }

ode_model<- function(t, y, parameters){
  with(
    as.list(c(y, parameters)),{
      dContamination <- Contamination*(1-Contamination/C)*r -d * exp(-g * t) * Contamination
      return(list(dContamination))
    }
  )
}
```

## Deterministic run function

```{r deterministic run function}

deterministic_run<-function(precision,initial_contamination,parameters){
  tmax = 24
  times = seq(0, tmax, length.out = precision) #precision+1

#sim=odeint(ode_model,initial_contamination,times,args=(r,C,d,g,l))
#parameters=c(r,C,d,g,l)
sim<- ode(initial_contamination, times, ode_model, parameters, method = "radau",atol = 1e-4, rtol = 1e-4)
# num_at_0=sim[int(precision*0.1/50.0)]
num_at_1=sim[(precision*1/tmax),2]
num_at_2=sim[(precision*2/tmax),2]
num_at_4=sim[(precision*4/tmax),2]
num_at_6=sim[(precision*6/tmax),2]
# num_at_16=sim[(precision*16/tmax),2]
# num_at_18=sim[(precision*18/tmax),2]
# num_at_20=sim[(precision*20/tmax),2]
# num_at_22=sim[(precision*22/tmax),2]
# num_at_24=sim[(precision*24/tmax),2]

return(cbind(num_at_1,num_at_2,num_at_4,num_at_6))
# return(cbind(num_at_2,num_at_4,num_at_6,num_at_16,num_at_18,num_at_20,num_at_22,num_at_24))
}
```

# Compiled C version
```{r Compiled C ODE}
deterministic_run_C<-function(precision,initial_contamination,parameters){
  tmax = 24
  times = seq(0, tmax, length.out = precision) #precision+1

Y <-c(Contamination=initial_contamination) 
sim <- ode(initial_contamination, times, func = "derivs", dllname = "testODE_arm64", initfunc = "initmod",
            parms = parameters, method = "radau", atol = 1e-4, rtol = 1e-4)

# num_at_0=sim[int(precision*0.1/50.0)]
num_at_1=sim[(precision*1/tmax),2]
num_at_2=sim[(precision*2/tmax),2]
num_at_4=sim[(precision*4/tmax),2]
num_at_6=sim[(precision*6/tmax),2]
num_at_16=sim[(precision*16/tmax),2]
num_at_18=sim[(precision*18/tmax),2]
num_at_20=sim[(precision*20/tmax),2]
num_at_22=sim[(precision*22/tmax),2]
num_at_24=sim[(precision*24/tmax),2]
return(cbind(num_at_1,num_at_2,num_at_4,num_at_6,num_at_16,num_at_18,num_at_20,num_at_22,num_at_24))
}
```

Initial data to input into the algorithm
```{r}
df %>% 
  group_by(treatment,group,time) %>% 
  summarise(M=mean(conc),S=sd(conc)) %>% 
  filter(treatment=="Treated" ) ->data

df %>% 
  group_by(treatment,group,time) %>% 
  summarise(M=mean(conc),S=sd(conc)) %>% 
  filter(treatment=="Untreated" ) ->dataUntreated
```



# Run the ABC Rejection Code - C version

```{r}
##################################################################################################################
## Applying the ABC rejection algorithm

# Treated
initial_contamination=c(Contamination=data$M[1])#20560)#data$M[1]) #median 34
experimental_data = data$M[1:NROW(data)] #c(1200,134,202,294)#c(11.5,5,2,0,6)#
s=data$S[1:NROW(data)]#c(93.70165,86.13942,162.11107,116.61904,123.61230,396.88789,628.87201,1147.13556)

# Untreated
initial_contaminationU=c(Contamination=dataUntreated$M[1])
experimental_dataU = dataUntreated$M[1:NROW(dataUntreated)] 
sU=dataUntreated$S[1:NROW(dataUntreated)]

sample_size = 1000
parameter_sample<- matrix(data=NA,nrow=1,ncol=5)
total_trials=0  #Starts the counter of run trials
accepted_trials=0 

#Create empty vector for the distances
distances<-c()
delta=60000#0.006136007
precision=5000 #this is delta t
# library(foreach)
# library(doParallel)
# cores=detectCores()
# cl <- makeCluster(cores[1]-1) #not to overload your computer
# registerDoParallel(cl)
while (NROW(parameter_sample) <= sample_size){
#foreach(total_trials=1:2, .packages=c("deSolve")) %dopar% {
  

  # Define parameters ----------
  # The prior distributions we use are d ~ U(0.001,10.0), C ~ U(200,1200), r ~ U(0.001,1.0), g ~ U(0.001,1.0). 
  # We begin by sampling from these distributions and simulating the process
  trial_r = runif(1,1E-1,1000)
  trial_rU = runif(1,1E-1,1000)
  trial_C = runif(1,1e2,1e6)
  # trial_CU = runif(1,1e2,1e6)
  trial_d = runif(1,0.0001,10)
  trial_g = runif(1,0.0001,10)
  # trial_l = runif(1,1E-5,1e2)
  total_trials=total_trials+1.0
#print(total_trials)
# parameters=c(r=trial_r,C=trial_C,d=trial_d,g=trial_g) #For R code - but it doesn't matter what order they are in.
# one_run = deterministic_run(precision,initial_contamination,parameters)# R code

parameters <- c(C = trial_C, d = trial_d, g=trial_g,r=trial_r); #For C code
#Treated
one_run = deterministic_run_C(precision,initial_contamination,parameters)# For C code
one_runU = deterministic_run_C(precision,initial_contaminationU,c(C = trial_C, d = 0, g=0,r=trial_rU))# For C code

# Now we find the Euclidean distance between the simulated output and the
# experimental results. delta is the threshold that the Euclidean distance
# must be less than for us to accept the trial parameters into our sample.

euclidean_distance = abs(dist(rbind(one_run,experimental_data)))+abs(dist(rbind(one_runU,experimental_dataU)))#Distance(one_run, experimental_data,s)+Distance(one_runU, experimental_dataU,sU) # abs(dist(rbind(one_run,experimental_data))) #

#print(euclidean_distance)# print(parameter_sample,euclidean_distance)
#print(total_trials)

if (euclidean_distance < delta){
  #parameter_sample = parameter_sample[!is.na(parameter_sample)];
  # print(euclidean_distance)
  parameter_sample=rbind(parameter_sample, c(trial_C,trial_d,trial_g,trial_r,trial_rU))
  distances=rbind(distances,euclidean_distance)
  accepted_trials=accepted_trials+1.0
  print(paste0("Trial number accepted: ",accepted_trials))
}
  #else{
  #  print(euclidean_distance)
  #  }

}


print(paste0("Percentage of trials accepted: ",(100*accepted_trials/total_trials)))#Order the distances and then the parameter sample
parameter_sample<-parameter_sample[order(distances), ]
distances <- distances[order(distances)]
parameter_sample<-parameter_sample%>%as.data.frame()#%>%set_colnames(c("r", "C", "d","g","l"))
colnames(parameter_sample)<-c( "C", "d","g","r","rU")
summary(distances)
#stopImplicitCluster()


write.csv(parameter_sample,"../Data/parameter_sample.csv",row.names = FALSE)
write.csv(distances,"../Data/distances.csv",row.names = FALSE)

```

# Parallel version - Non-C
This is the parallel implementation usingg the furrrr framework with 4 worker cores
```{r parallel implementation with furrr}
##################################################################################################################
## Applying the ABC rejection algorithm

# library(foreach)
# library(doParallel)
# cores=detectCores()
# cl <- makeCluster(cores[1]-1) #not to overload your computer
# registerDoParallel(cl)

simulate_function<-function(a){
  # set.seed(seed = TRUE)
  .Random.seed <- a
  #set up empty distances vector
  distances<-c()
  precision=5000 #this is delta t
  
  #Experimental data
  initial_contamination=c(Contamination=20560) #median 34
  experimental_data = c(1200,134,202,294)#experimental_data = c(134,202,294,400,644,1232,2044,2868)#c(11.5,5,2,0,6)#
  s=c(93.70165,86.13942,162.11107,116.61904,123.61230,396.88789,628.87201,1147.13556)
  sample_size = 200
  #parameter_sample <- c()
  parameter_sample<- matrix(data=NA,nrow=1,ncol=4)
  total_trials=0  #Starts the counter of run trials
  accepted_trials=0 #Starts the counter for accepted trial numbers, don't change


  #Loop to sample values until met the sample size
while (NROW(parameter_sample) <= sample_size){
#foreach(total_trials=1:2, .packages=c("deSolve")) %dopar% {
  
  # The prior distributions we use are d ~ U(0.001,10.0), C ~ U(200,1200), r ~ U(0.001,1.0), g ~ U(0.001,1.0). 
  # We begin by sampling from these distributions and simulating the process
trial_r = runif(1,1E-1,1e2)
trial_C = runif(1,1e3,6e3)
trial_d = runif(1,1,2e2)
trial_g = runif(1,1,2e2)
# trial_l = runif(1,1E-5,1e2)
total_trials=total_trials+1.0
#print(total_trials)
parameters=c(r=trial_r,C=trial_C,d=trial_d,g=trial_g) # For R code
# parameters <- c(C = trial_C, d = trial_d, g=trial_g,r=trial_r); #For C code
one_run = deterministic_run(precision,initial_contamination,parameters)#

#experimental_data_noP2 = [652.0, 556.0, 424.5, 467.5, 428.0, 550.0, 672.0]

# Now we find the Euclidean distance between the simulated output and the
# experimental results. delta is the threshold that the Euclidean distance
# must be less than for us to accept the trial parameters into our sample.
delta = 1000#dictates distance
euclidean_distance = abs(dist(rbind(one_run,experimental_data)))#Distance(one_run, experimental_data,s)# #Distance(one_run, experimental_data,s)#_noP2)
#print(euclidean_distance)# print(parameter_sample,euclidean_distance)
#print(total_trials)

if (euclidean_distance < delta){
  #parameter_sample = parameter_sample[!is.na(parameter_sample)];
  parameter_sample=rbind(parameter_sample, c(trial_r,trial_C,trial_d,trial_g))
  distances=rbind(distances,euclidean_distance)
  accepted_trials=accepted_trials+1.0
  # print(paste0("Trial number accepted: ",accepted_trials))
}
  #else{
  #  print(euclidean_distance)
  #  }

}
  
# parameter_sample<-cbind(parameter_sample,distances)
parameter_sample<-parameter_sample[order(distances), ]
parameter_sample<-parameter_sample%>%as.data.frame()#%>%set_colnames(c("r", "C", "d","g","l"))
colnames(parameter_sample)<-c("r", "C", "d","g")
# return(parameter_sample)
print(paste("Summary of distances: ",distances %>% summary()))
print(paste("Percentage of accepted trials: ",accepted_trials/total_trials*100))

return(parameter_sample)
# return(list("parameter_sample" = parameter_sample, "distances" = distances))
}

# Parallel implementation

# print(paste0("Percentage of trials accepted: ",(100*accepted_trials/total_trials)))
# #Order the distances and then the parameter sample
# parameter_sample<-parameter_sample[order(distances), ]
# parameter_sample<-parameter_sample%>%as.data.frame()#%>%set_colnames(c("r", "C", "d","g","l"))
# colnames(parameter_sample)<-c("r", "C", "d","g","l")

#2 %>% purrr::rerun()
# Run in parallel

plan(multisession, workers = 6)
# temp.list<-lapply(X = 1:4,FUN = simulate_function)

RNGkind("L'Ecuyer-CMRG")
set.seed(42)
seeds <- list(.Random.seed)
temp.list<-future_map(.x = 1:6,.f = simulate_function)

# Reset to running sequentially
plan(sequential)


temp.list

temp.list %>%
  bind_rows() %>% 
  # select(parameter_sample) %>% 
  drop_na() %>% 
  unnest()  ->parameter_sample
write.csv(parameter_sample,"../Data/parameter_sample.csv",row.names = FALSE)

```

# Extract distances and parameter samples - Parallel
```{r extract parameter samples}
# lapply(temp.list, function(x) x[(names(x) %in% c("r", "C","d","g"))])->tst
# pp %>% purrr::pluck(1)

temp.list %>%
  bind_rows() %>% 
  # select(parameter_sample) %>% 
  drop_na() %>% 
  pivot_longer(c(r,C,d,g)) %>% 
  ggplot()+
  geom_histogram(aes(x=value,fill=name))+
  facet_wrap(~name,scales="free")+
  hrbrthemes::theme_ipsum()





```


# Plot histograms
```{r histograms}
parameter_sample %>% 
  pivot_longer(c(C,d,g,r)) %>% 
  ggplot()+
  geom_density(aes(x=value,fill=name))+
  scale_fill_brewer(palette = "Set1")+
  facet_wrap(~name,scales="free",ncol = 4)+
  hrbrthemes::theme_ipsum()+
  theme(legend.position = "none")->b


b
write.csv(parameter_sample,"../Data/parameter_sample.csv",row.names = FALSE)
```

```{r}
parameter_sample %>% drop_na %>% summary()
```

# Plot example curves against experimetnal data
```{r curves}
## Plotting a single best curve
  # initial_contamination=c(Contamination=1200) #median 34
  # experimental_data = c(134,202,294,400,644,1232,2044,2868)#c(11.5,5,2,0,6)#
  # s=c(93.70165,86.13942,162.11107,116.61904,123.61230,396.88789,628.87201,1147.13556)

tmax = 24
times = seq(0, tmax, length.out = 5000+1)

# sim1<-ode(initial_contamination, times, ode_model, parameter_sample[1,], method = "radau",atol = 1e-4, rtol = 1e-4)
# sim2<-ode(initial_contamination, times, ode_model, parameter_sample[2,], method = "radau",atol = 1e-4, rtol = 1e-4)
# sim3<-ode(initial_contamination, times, ode_model, parameter_sample[3,], method = "radau",atol = 1e-4, rtol = 1e-4)
# sim4<-ode(initial_contamination, times, ode_model, parameter_sample[4,], method = "radau",atol = 1e-4, rtol = 1e-4)
# sim<-rbind(sim1[,c(1,2)],sim2[,c(1,2)],sim3[,c(1,2)],sim4[,c(1,2)])%>%as.data.frame()
# sim$CurveNum<-rep(1:4,each=nrow(sim1))
# #lines(sim,col="blue")
# sim<-sim%>%as.data.frame()#%>%tidyr::set_colnames(c("Time","Contamination","CurveNum"))
# colnames(sim)<-c("Time","Contamination","CurveNum")
# 
# 
# ggplot()+
#   geom_line(data=sim %>% as_tibble(),aes(x=Time,y=Contamination,colour=as.factor(CurveNum)))+
#   geom_point(data=df %>% filter(treatment=="Treated" & time <10),aes(x=time,y=conc))+
#   # stat_summary(data=df %>% filter(treatment=="Treated"),aes(x=time,y=conc),fun.data = "mean_cl_boot", colour = "red", size = 2)+
#   # scale_y_continuous(trans="log")+
#   scale_y_continuous(trans="log")+
#   hrbrthemes::theme_ipsum()+
#    theme(legend.position = "none")

parameter_sample %>% 
  drop_na() %>% 
  summarise(across(.cols=everything(),quantile)) ->p_summary

#Treated
# p_summary[4,]
sim1<-ode(initial_contamination, times, ode_model, parameter_sample[3,], method = "radau",atol = 1e-4, rtol = 1e-4)
sim2<-ode(initial_contamination, times, ode_model, parameter_sample[1,], method = "radau",atol = 1e-4, rtol = 1e-4)
sim3<-ode(initial_contamination, times, ode_model, parameter_sample[2,], method = "radau",atol = 1e-4, rtol = 1e-4)
# sim4<-ode(initial_contamination, times, ode_model, p_summary[1,], method = "radau",atol = 1e-4, rtol = 1e-4)
sim<-rbind(sim1[,c(1,2)],sim2[,c(1,2)],sim3[,c(1,2)])%>%as.data.frame()
sim$CurveNum<-rep(1:3,each=nrow(sim1))
#lines(sim,col="blue")
sim<-sim%>%as.data.frame()#%>%tidyr::set_colnames(c("Time","Contamination","CurveNum"))
colnames(sim)<-c("Time","Contamination","CurveNum")

ribbon_data <- data.frame(ymin=sim %>% as_tibble()%>% filter(CurveNum==1) %>% select(Time,Contamination),
                          ymax=sim %>% as_tibble()%>% filter(CurveNum==3) %>% select(Contamination)) %>% rename(Time=ymin.Time,ymin=ymin.Contamination,ymax=Contamination)
  


ggplot()+
  geom_point(data=df %>% filter(treatment=="Treated" & time <30),aes(x=time,y=conc),colour="darkblue")+
  geom_line(data=sim %>% as_tibble()%>% filter(CurveNum==2) ,aes(x=Time,y=Contamination))+
  geom_ribbon(data=ribbon_data ,aes(x=Time,ymin=ymin, ymax=ymax), alpha=0.2)+
scale_y_continuous(trans="log")+
  xlab("Time (h)")+
  ylab("Concentration")+
  hrbrthemes::theme_ipsum()+
  theme(legend.position = "none")->a

#Untreated
sim1<-ode(initial_contaminationU, times, ode_model, c(C=parameter_sample$C[1],d=0,g=0, r=parameter_sample$r[1],d=0), method = "radau",atol = 1e-4, rtol = 1e-4)
sim2<-ode(initial_contaminationU, times, ode_model, c(C=parameter_sample$C[1],d=0,g=0, r=parameter_sample$r[1],d=0), method = "radau",atol = 1e-4, rtol = 1e-4)
sim3<-ode(initial_contaminationU, times, ode_model, c(C=parameter_sample$C[1],d=0,g=0, r=parameter_sample$r[1],d=0), method = "radau",atol = 1e-4, rtol = 1e-4)
# sim4<-ode(initial_contamination, times, ode_model, p_summary[1,], method = "radau",atol = 1e-4, rtol = 1e-4)
sim<-rbind(sim1[,c(1,2)],sim2[,c(1,2)],sim3[,c(1,2)])%>%as.data.frame()
sim$CurveNum<-rep(1:3,each=nrow(sim1))
#lines(sim,col="blue")
sim<-sim%>%as.data.frame()#%>%tidyr::set_colnames(c("Time","Contamination","CurveNum"))
colnames(sim)<-c("Time","Contamination","CurveNum")

ribbon_data <- data.frame(ymin=sim %>% as_tibble()%>% filter(CurveNum==1) %>% select(Time,Contamination),
                          ymax=sim %>% as_tibble()%>% filter(CurveNum==3) %>% select(Contamination)) %>% rename(Time=ymin.Time,ymin=ymin.Contamination,ymax=Contamination)

ggplot()+
  geom_point(data=df %>% filter(treatment=="Untreated" & time <30),aes(x=time,y=conc),colour="darkred")+
  geom_line(data=sim %>% as_tibble()%>% filter(CurveNum==2) ,aes(x=Time,y=Contamination))+
  geom_ribbon(data=ribbon_data ,aes(x=Time,ymin=ymin, ymax=ymax), alpha=0.2)+
scale_y_continuous(trans="log")+
  xlab("Time (h)")+
  ylab("Concentration")+
  hrbrthemes::theme_ipsum()+
  theme(legend.position = "none")->c

ggpubr::ggarrange(a,c,nrow=2,labels = c("Treated","Untreated"))


```


# GGally - GGpairs


```{r}

parameter_sample %>% 
   filter(row_number() %% 10 == 1) %>% 
  GGally::ggpairs()+
  hrbrthemes::theme_ipsum()
```

# TODO Modelar segundo cacho y ver si r explica el incremento
# TODO probar otra ecuacion

# Define the ODE to solve 

1) We consider only recontamination from surface contacts, not from additional proliferation of the bacterias on the hands.
$$ y'=(1-\dfrac{y}{C})r*y-d\exp(-g t)y$$

$C$ is carrying capacity. $d$ is maximum effecicacy. $g$ is decay from the disinfectant. $r$ recontamination from contacts (this is linear). $l$ is the proliferation rate on the hands.

2) If we wanted to consider proliferation on hands instead of surface accrual then we could use:
$$ y'=y(1-\dfrac{y}{C})l-d\exp(-g t)y$$

where $l$ is the rate at which microorganisms proliferate on hands. E.g. reproduction.

3) Proliferation and increase from contacts can be combined:
$$ y'=(yl+r)(1-\dfrac{y}{C})-d\exp(-g t)y$$

But since we don't see any saturation then we could ignore carrying capacity. 
$$ y'=(yl+r)(1-y)-d\exp(-g t)y$$

However with the data we have it is unlikely that we can learn about $l$ and $r$ together.

Currently we only consider the first 4 experimental datapoints as there is a gap which is hard to explain using this model-

Notes: 12th April
- We can first try to fit the model to the data from group 2's washed hand to learn only about $C$ and $r$.
- Then with those learned $C$ and $r$ we can fit the model to the data from group 1's washed hand to learn only about $d$ and $g$. 
- But we never reach a saturation so maybe we shouldn't consider $C$ at all.


Notes: 13th April
- Try to take into account untreated data from group 1 to estimate the effect of proliferation and accrual
- Apply $ y'=(yl+r)(1-y)$ to untreated hand from group 1 to learn about $l$ (proliferation rate) and $r$ (surface contact rate).
- Ideally we use all data from both groups to learn about all the parametrs because we can assume that the untreated hand from both groups behaves similarly. Fitting an exponential model to these data, suggests $l$ (proliferation) is the same for both groups (0.18)
$$ y'=yl-d\exp(-g t)y$$
Notes 29th April

Consider a two-part linear model to capture drop and then increase. Take into consideration the fallow time / sleep


